import pandas as pd
import seaborn as sns

import matplotlib.pyplot as plt

# Load your dataset
file_path = "/Users/ajanthanjoseph/Documents/GitHub/CIND820/.diabetes_012_health_indicators_BRFSS2015.csv.icloud"
df = pd.read_csv(file_path)

# Create interaction features
df["BMI_Age"] = df["BMI"] * df["Age"]
df["HighBP_HighChol"] = df["HighBP"] * df["HighChol"]
df["PhysActivity_BMI"] = df["PhysActivity"] * df["BMI"]

# Display the first few rows of the transformed dataset
df.head()

from scipy.stats import chi2_contingency

# Select categorical variables to test against "Diabetes_012"
categorical_features = [
    "HighBP", "HighChol", "CholCheck", "Smoker", "Stroke",
    "HeartDiseaseorAttack", "PhysActivity", "Fruits", "Veggies",
    "HvyAlcoholConsump", "AnyHealthcare", "NoDocbcCost", "GenHlth",
    "DiffWalk", "Sex", "Age", "Education", "Income", "BMI_Age", "HighBP_HighChol", "PhysActivity_BMI"
]

# Perform chi-square test for each categorical feature
chi_square_results = {}
for feature in categorical_features:
    contingency_table = pd.crosstab(df[feature], df["Diabetes_012"])
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    chi_square_results[feature] = {"Chi2": chi2, "p-value": p}

# Convert results to a DataFrame
chi_square_df = pd.DataFrame.from_dict(chi_square_results, orient="index")
chi_square_df.sort_values(by="p-value", inplace=True)

# Display the results
chi_square_df

# Separate features and the target variable
X = df.drop(columns=['Diabetes_012'])
y = df['Diabetes_012']

# Re-import necessary libraries and redefine the ANOVA selector
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from sklearn.preprocessing import StandardScaler

# Apply Chi-Squared test for feature selection
chi2_selector = SelectKBest(score_func=chi2, k='all')
chi2_selector.fit(X, y)
chi2_scores = chi2_selector.scores_

# Apply ANOVA F-test for feature selection
anova_selector = SelectKBest(score_func=f_classif, k='all')
anova_selector.fit(X, y)
anova_scores = anova_selector.scores_

# Create a DataFrame with the feature selection results
feature_selection_results = pd.DataFrame({
    'Feature': X.columns,
    'Chi2 Score': chi2_scores,
    'ANOVA F Score': anova_scores
}).sort_values(by=['Chi2 Score', 'ANOVA F Score'], ascending=False)

# Display the results to the user
print(feature_selection_results)

# Identify the 6 lowest scoring features based on the combined scores
lowest_features = feature_selection_results.nsmallest(6, ['Chi2 Score', 'ANOVA F Score'])['Feature'].tolist()

# Drop these features from the dataset
X_reduced = X.drop(columns=lowest_features)

# Display the updated dataset to the user
print(X_reduced)
# The display() function is a built-in way to show DataFrames in Jupyter notebooks.
# This assumes the user just wants to display the DataFrame X_reduced.
# If the user needs specific formatting, consider using X_reduced.to_string() or other display options.

# Standardize numerical columns
numerical_features = ["BMI", "Age", "BMI_Age", "PhysActivity_BMI"]
scaler = StandardScaler()
df[numerical_features] = scaler.fit_transform(df[numerical_features])

from imblearn.over_sampling import SMOTE

# Apply SMOTE to balance the target classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_reduced, y)

# Check the distribution of the target variable after SMOTE
print(y_resampled.value_counts())

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets with a 70-30 split
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled
)

# Display the shapes of the resulting datasets
print("Training and Testing Dataset Shapes:")
print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             roc_auc_score, confusion_matrix, ConfusionMatrixDisplay,
                             precision_recall_curve, roc_curve, auc)
import matplotlib.pyplot as plt
import seaborn as sns

# Separate features and the target variable
X = df.drop(columns=['Diabetes_012'])
y = df['Diabetes_012']

# Define models without hyperparameter tuning
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'CatBoost': CatBoostClassifier(verbose=0),
    'XGBoost': XGBClassifier()
}

# Define hyperparameters for GridSearchCV
param_grids = {
    'Logistic Regression': {'C': [0.1, 1, 10]},
    'Decision Tree': {'max_depth': [5, 10, 20]},
    'Random Forest': {'n_estimators': [50, 100, 200]},
    'Gradient Boosting': {'n_estimators': [50, 100, 200]},
    'CatBoost': {'iterations': [50, 100, 200]},
    'XGBoost': {'n_estimators': [50, 100, 200]}
}

# Evaluation function
def evaluate_model(y_test, y_pred, y_pred_proba, model_name):
    print(f'\nModel: {model_name}')
    print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')
    print(f'Precision: {precision_score(y_test, y_pred, average="weighted"):.4f}')
    print(f'Recall: {recall_score(y_test, y_pred, average="weighted"):.4f}')

    # Calculate ROC AUC for multiclass
    # Use predict_proba to get probabilities for all classes
    y_pred_proba_all_classes = model.predict_proba(X_test)

    print(f'ROC AUC: {roc_auc_score(y_test, y_pred_proba_all_classes, multi_class="ovr"):.4f}')  # Use probabilities for all classes
    cm = confusion_matrix(y_test, y_pred)
    ConfusionMatrixDisplay(cm).plot()
    plt.title(f'Confusion Matrix: {model_name}')
    plt.show()

    # Precision-Recall Curve (for class 1 - adjust if needed)
    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_all_classes[:, 1], pos_label=1)
    plt.plot(recall, precision, marker='.', label=model_name)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.show()

    # ROC Curve (for class 1 - adjust if needed)
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba_all_classes[:, 1], pos_label=1)
    plt.plot(fpr, tpr, marker='.', label=f'{model_name} (AUC = {auc(fpr, tpr):.4f})')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()

    # Evaluate models without hyperparameter tuning
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    evaluate_model(y_test, y_pred, y_pred_proba, f'{name} (No Tuning)')