import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
import numpy as np

# Load the dataset
file_path = '/Users/ajanthanjoseph/Documents/GitHub/CIND820/diabetes_012_health_indicators_BRFSS2015.csv' 
df = pd.read_csv(file_path)

# Feature engineering
df['BMI_Age'] = df['BMI'] * df['Age']
df['HighBP_HighChol'] = df['HighBP'] * df['HighChol']
df['PhysActivity_BMI'] = df['PhysActivity'] * df['BMI']

# Standardize numerical features
numerical_features = ['BMI', 'Age', 'BMI_Age', 'PhysActivity_BMI']
scaler = StandardScaler()
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Prepare features and target variable
X = df.drop(columns=['Diabetes_012'])
y = df['Diabetes_012']

# Define cross-validation strategy
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Function to evaluate models with cross-validation manually computing precision/recall
def cross_validate_model(model, X, y, model_name):
    accuracies, precisions, recalls, roc_aucs = [], [], [], []
    
    for train_idx, test_idx in cv.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        # Handle probability predictions safely
        if hasattr(model, "predict_proba"):
            y_pred_proba = model.predict_proba(X_test)
            
            # Ensure proper handling of binary vs. multi-class cases
            if y_pred_proba.shape[1] == 2:  # Binary classification
                y_pred_proba = y_pred_proba[:, 1]
                roc_auc = roc_auc_score(y_test, y_pred_proba)
            elif y_pred_proba.shape[1] > 2:  # Multi-class classification
                roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
            else:
                roc_auc = np.nan
        else:
            roc_auc = np.nan  # Set to NaN if model doesn't support predict_proba
        
        accuracies.append(accuracy_score(y_test, y_pred))
        precisions.append(precision_score(y_test, y_pred, average='weighted', zero_division=1))
        recalls.append(recall_score(y_test, y_pred, average='weighted', zero_division=1))
        roc_aucs.append(roc_auc)
    
    print(f'\nModel: {model_name} (Cross-Validation)')
    print(f'Accuracy: {np.mean(accuracies):.2f}')
    print(f'Precision: {np.mean(precisions):.2f}')
    print(f'Recall: {np.mean(recalls):.2f}')
    print(f'ROC AUC: {np.nanmean(roc_aucs):.2f}')

# Model configurations
models = {
    'Logistic Regression': LogisticRegression(max_iter=200, solver='liblinear'),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'Decision Tree': DecisionTreeClassifier(max_depth=10),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100),
    'CatBoost': CatBoostClassifier(verbose=0, iterations=100)
}

# Evaluate models using cross-validation on unbalanced data
for name, model in models.items():
    cross_validate_model(model, X, y, name)

# Apply SMOTE for balancing
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Evaluate models using cross-validation on balanced data
for name, model in models.items():
    cross_validate_model(model, X_resampled, y_resampled, f'{name} (Balanced Data)')

# Hyperparameter tuning using GridSearchCV with sensitivity analysis
param_grid = {
    'Logistic Regression': {'C': [0.01, 0.1, 1, 10, 100]},
    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 20]},
    'Decision Tree': {'max_depth': [5, 10, 20], 'min_samples_split': [2, 5, 10]},
    'Gradient Boosting': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]},
    'XGBoost': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]},
    'CatBoost': {'iterations': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}
}

sensitivity_results = {}

for name, model in models.items():
    grid = GridSearchCV(model, param_grid.get(name, {}), cv=cv, scoring='accuracy', n_jobs=-1, verbose=1)
    grid.fit(X_resampled, y_resampled)
    best_model = grid.best_estimator_
    cross_validate_model(best_model, X_resampled, y_resampled, f'{name} (Tuned with GridSearch)')
    sensitivity_results[name] = grid.cv_results_

# Print sensitivity analysis results
for model_name, results in sensitivity_results.items():
    print(f'Sensitivity Analysis for {model_name}:')
    for param, mean_score in zip(results['params'], results['mean_test_score']):
        print(f'Parameters: {param}, Mean Accuracy: {mean_score:.2f}')
